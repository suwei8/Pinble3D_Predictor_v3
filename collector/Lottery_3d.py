# collector/pinble_scraper.py

import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import time

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
DATA_PATH = os.path.join(BASE_DIR, "data", "3d_shijihao_history.csv")

URL = "http://134.175.237.107:8902/LotteryOneList3D.aspx?name=3D"
HEADERS = {
    "User-Agent": "Mozilla/5.0",
    "Referer": "http://www.pinble.com/",
}


def get_viewstate_and_eventvalidation(session, html):
    soup = BeautifulSoup(html, "html.parser")
    viewstate = soup.find("input", id="__VIEWSTATE")["value"]
    event_node = soup.find("input", id="__EVENTVALIDATION")
    eventvalidation = event_node["value"] if event_node else ""
    return viewstate, eventvalidation


def scrape_page(session, page_num, viewstate, eventvalidation):
    data = {
        "__EVENTTARGET": "AspNetPager1",
        "__EVENTARGUMENT": str(page_num),
        "__VIEWSTATE": viewstate,
    }
    if eventvalidation:
        data["__EVENTVALIDATION"] = eventvalidation

    resp = session.post(URL, headers=HEADERS, data=data, timeout=10)
    resp.encoding = "gb2312"
    soup = BeautifulSoup(resp.text, "html.parser")

    new_viewstate, new_eventvalidation = get_viewstate_and_eventvalidation(session, resp.text)

    table = soup.find("table", id="MyGridView")
    if not table:
        raise RuntimeError(f"âŒ ç¬¬ {page_num} é¡µæœªæ‰¾åˆ°è¡¨æ ¼")

    rows = table.find_all("tr")[1:]
    data_rows = []
    for row in rows:
        cols = row.find_all("td")
        if len(cols) >= 4:
            date = cols[0].get_text(strip=True)
            issue = cols[1].get_text(strip=True)
            sim_test = cols[2].get_text(strip=True).zfill(3)
            open_code = cols[3].get_text(strip=True)
            data_rows.append([date, issue, sim_test, open_code])
    return data_rows, new_viewstate, new_eventvalidation

# ç‰¹å¾å¤„ç†æ¨¡å—
def enrich_features(df):
    df["sim_digit_1"] = df["sim_test_code"].str[0].astype(int)
    df["sim_digit_2"] = df["sim_test_code"].str[1].astype(int)
    df["sim_digit_3"] = df["sim_test_code"].str[2].astype(int)

    df["open_digit_1"] = df["open_code"].str[0].astype(int)
    df["open_digit_2"] = df["open_code"].str[1].astype(int)
    df["open_digit_3"] = df["open_code"].str[2].astype(int)

    df["sim_sum_val"] = df[["sim_digit_1", "sim_digit_2", "sim_digit_3"]].sum(axis=1)
    df["sim_span"] = df[["sim_digit_1", "sim_digit_2", "sim_digit_3"]].max(axis=1) - df[["sim_digit_1", "sim_digit_2", "sim_digit_3"]].min(axis=1)
    df["sim_pattern"] = df.apply(lambda row: "ç»„ä¸‰" if len(set([row.sim_digit_1, row.sim_digit_2, row.sim_digit_3])) == 2 else ("è±¹å­" if len(set([row.sim_digit_1, row.sim_digit_2, row.sim_digit_3])) == 1 else "ç»„å…­"), axis=1)

    df["open_sum_val"] = df[["open_digit_1", "open_digit_2", "open_digit_3"]].sum(axis=1)
    df["open_span"] = df[["open_digit_1", "open_digit_2", "open_digit_3"]].max(axis=1) - df[["open_digit_1", "open_digit_2", "open_digit_3"]].min(axis=1)
    df["open_pattern"] = df.apply(lambda row: "ç»„ä¸‰" if len(set([row.open_digit_1, row.open_digit_2, row.open_digit_3])) == 2 else ("è±¹å­" if len(set([row.open_digit_1, row.open_digit_2, row.open_digit_3])) == 1 else "ç»„å…­"), axis=1)

    def count_match(row):
        sim_digits = {row.sim_digit_1, row.sim_digit_2, row.sim_digit_3}
        open_digits = {row.open_digit_1, row.open_digit_2, row.open_digit_3}
        return len(sim_digits & open_digits)

    def count_pos_match(row):
        return int(row.sim_digit_1 == row.open_digit_1) + int(row.sim_digit_2 == row.open_digit_2) + int(row.sim_digit_3 == row.open_digit_3)

    df["match_count"] = df.apply(count_match, axis=1)
    df["match_pos_count"] = df.apply(count_pos_match, axis=1)

    return df

def scrape_all_pages(max_pages=208, delay=1.0):
    # âœ… åŠ è½½å·²æœ‰æ•°æ®
    if os.path.exists(DATA_PATH):
        try:
            df_existing = pd.read_csv(DATA_PATH, dtype={"issue": str, "sim_test_code": str, "open_code": str})
            existing_issues = set(df_existing["issue"])
            print(f"ğŸ“¦ å·²åŠ è½½å†å²æ•°æ® {len(existing_issues)} æ¡")
        except Exception as e:
            print(f"âš ï¸ å†å²æ•°æ®æ–‡ä»¶æŸåæˆ–ç©ºæ–‡ä»¶ï¼Œå¿½ç•¥åŠ è½½ï¼š{e}")
            df_existing = pd.DataFrame(columns=["date", "issue", "sim_test_code", "open_code"])
            existing_issues = set()
    else:
        df_existing = pd.DataFrame(columns=["date", "issue", "sim_test_code", "open_code"])
        existing_issues = set()


    session = requests.Session()
    init_resp = session.get(URL, headers=HEADERS, timeout=60)
    init_resp.encoding = "gb2312"
    viewstate, eventvalidation = get_viewstate_and_eventvalidation(session, init_resp.text)

    new_data = []
    for page in range(1, max_pages + 1):
        try:
            print(f"ğŸ“„ æ­£åœ¨æŠ“å–ç¬¬ {page} é¡µ")
            page_data, viewstate, eventvalidation = scrape_page(session, page, viewstate, eventvalidation)

            # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„ issue
            page_data_filtered = [row for row in page_data if row[1] not in existing_issues]

            if page_data_filtered:
                new_data.extend(page_data_filtered)
                print(f"â• æœ¬é¡µæ–°å¢ {len(page_data_filtered)} æ¡")
            else:
                print(f"â© æœ¬é¡µæ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡")

            time.sleep(delay)
        except Exception as e:
            print(f"âš ï¸ ç¬¬ {page} é¡µæŠ“å–å¤±è´¥ï¼š{e}")
            continue

    df_new = pd.DataFrame(new_data, columns=["date", "issue", "sim_test_code", "open_code"])
    df_new["sim_test_code"] = df_new["sim_test_code"].astype(str).str.zfill(3)

    # åˆå¹¶æ–°æ—§æ•°æ®
    df_all = pd.concat([df_existing, df_new], ignore_index=True)
    df_all = df_all.drop_duplicates(subset=["issue"])
    df_all = df_all.sort_values(by="issue", ascending=True).reset_index(drop=True)
    return df_all


def save_to_csv(df):
    os.makedirs(os.path.dirname(DATA_PATH), exist_ok=True)
    df.to_csv(DATA_PATH, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²ä¿å­˜åˆ° {DATA_PATH}")


if __name__ == "__main__":
    df_raw = scrape_all_pages(max_pages=1)               # æŠ“åŸå§‹æ•°æ®
    # df_raw = scrape_all_pages()  # é»˜è®¤ max_pages=208ï¼Œé‡‡é›†å…¨éƒ¨é¡µ
    save_to_csv(df_raw)                                   # âœ… åŸå§‹æ•°æ®å•ç‹¬ä¿å­˜
    df = enrich_features(df_raw.copy())                   # ç‰¹å¾å¢å¼ºï¼Œé¿å…æ±¡æŸ“åŸå§‹æ•°æ®
    df.to_csv(os.path.join(BASE_DIR, "data", "3d_shijihao_features.csv"), index=False, encoding="utf-8-sig")
    print("âœ… 3d_shijihao_features.csv ç”Ÿæˆå®Œæ¯•")